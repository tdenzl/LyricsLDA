{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "professional-instruction",
   "metadata": {},
   "source": [
    "# LDA Topic Modeling on Song Lyrics\n",
    "## Creating a Social Network Graph of the Marvel Universe\n",
    "\n",
    "\n",
    "This notebooks performs LDA Topic Modeling on Song Lyrics [(see data source)](https://www.kaggle.com/albertsuarez/azlyrics).\n",
    "\n",
    "**Author:** Tim Denzler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "indoor-sculpture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "#execute the following of not downloaded already:\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "japanese-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "langs = detect_langs('Otec matka syn.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-worst",
   "metadata": {},
   "source": [
    "## Step 1: Read CSV file and identify language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pacific-kernel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [18:30<00:00, 41.12s/it]\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect_langs\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "lyric_corpus = []\n",
    "lang_filtered = 0\n",
    "for filename in tqdm(os.listdir('azlyrics-scraper')):\n",
    "    path = './azlyrics-scraper/'+filename\n",
    "    with open(path, 'r') as f:\n",
    "        data = csv.reader(f)\n",
    "        headers = next(data) #skip headers\n",
    "        for row in data:\n",
    "            if len(row)< 5: #lyrics column not included\n",
    "                continue\n",
    "            if not row[4]: #lyrics are empty\n",
    "                continue\n",
    "            try:\n",
    "                langlist = detect_langs(row[4])\n",
    "                for language in langlist:\n",
    "                    if language.prob < 0.95 or language.lang != 'en': #lyrics not clearly identified as English\n",
    "                        lang_filtered += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        lyric_corpus.append(row[4])\n",
    "            except LangDetectException:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efficient-monitor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 0 songs that had lyrics identified in a language other than English. 123794 songs remain for topic modeling\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtered\", lang_filtered, \"songs that had lyrics identified in a language other than English.\", len(lyric_corpus), \"songs remain for topic modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "favorite-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fileObject = open(\"./data/lyric_corpus\",'wb') \n",
    "pickle.dump(lyric_corpus,fileObject)   \n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-certificate",
   "metadata": {},
   "source": [
    "## Step 2: Tokenization of Song Lyrics\n",
    "\n",
    "All lyrics are currently stored as strings. First of all, each lyrics's text in the corpus is converted to lowercase letters.\n",
    "The text strings are tokenized in order continue the text processing using the *RegexpTokenizer* and subsequently stored in *lyric_corpus_tokenized* for each paper object. Each word ('\\w' refers to word characters, so alphanumerics) is now a string and each paper a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "injured-resolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123794/123794 [00:11<00:00, 10342.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "lyric_corpus_tokenized = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for lyric in tqdm(lyric_corpus):\n",
    "    tokenized_lyric = tokenizer.tokenize(lyric.lower())  #tokenize and lower each lyric\n",
    "    lyric_corpus_tokenized.append(tokenized_lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "overall-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_token_in_corpus (corpus): #returns number of tokens for any given corpus\n",
    "    return_count = 0\n",
    "    for song in corpus:\n",
    "        return_count += len(song)\n",
    "    print(\"Total of\", return_count, \"tokens in the current corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "facial-minority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 31455368 tokens in the current corpus.\n"
     ]
    }
   ],
   "source": [
    "count_token_in_corpus(lyric_corpus_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-guyana",
   "metadata": {},
   "source": [
    "## Step 3: Removing Numeric and Single Character Tokens\n",
    "\n",
    "Tokens that only contain numbers or consist of only one letter are removed to reduce the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "sensitive-gross",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123794it [00:06, 19741.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for s,song in tqdm(enumerate(lyric_corpus_tokenized)):\n",
    "    filtered_song = []    \n",
    "    for token in song:\n",
    "        if len(token) > 2 and not token.isnumeric():\n",
    "            filtered_song.append(token)\n",
    "    lyric_corpus_tokenized[s] = filtered_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eight-thriller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 22178517 tokens in the current corpus.\n"
     ]
    }
   ],
   "source": [
    "count_token_in_corpus(lyric_corpus_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-injury",
   "metadata": {},
   "source": [
    "## Step 4: Token Lemmatization\n",
    "*NLTK's WordNetLemmatizer* is imported, lemmatizing each token. This means each word is reduced to its stem or base form in order to enable better comparability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "corresponding-freedom",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123794it [01:16, 1619.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for s,song in tqdm(enumerate(lyric_corpus_tokenized)):\n",
    "    lemmatized_tokens = []\n",
    "    for token in song:\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(token))\n",
    "    lyric_corpus_tokenized[s] = lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "nominated-liability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 22178517 tokens in the current corpus.\n"
     ]
    }
   ],
   "source": [
    "count_token_in_corpus(lyric_corpus_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-resolution",
   "metadata": {},
   "source": [
    "## Step 5: Remove Stop Words and Profanities\n",
    "In order to further reduce dimensionality, all words holding little to no value for topic modeling are removed. For this, *NLTK's stopwords*, a list of common stop words in the English language is imported and a few words that occured in previous modeling attempts are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "russian-athletics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123794it [00:59, 2081.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "new_stop_words = ['ooh','yeah','hey','whoa','woah', 'ohh', 'was', 'mmm', 'oooh','yah','yeh','mmm', 'hmm','deh','doh','jah','wa']\n",
    "stop_words.extend(new_stop_words)\n",
    "\n",
    "for s,song in tqdm(enumerate(lyric_corpus_tokenized)):\n",
    "    filtered_text = []    \n",
    "    for token in song:\n",
    "        if token not in stop_words:\n",
    "            filtered_text.append(token)\n",
    "    lyric_corpus_tokenized[s] = filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "compliant-tiger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 13787937 tokens in the current corpus.\n"
     ]
    }
   ],
   "source": [
    "count_token_in_corpus(lyric_corpus_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-tender",
   "metadata": {},
   "source": [
    "Filter out profanities based on a predefined list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ranging-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "profanities = []\n",
    "with open('profanity.txt', 'r') as file:\n",
    "    prof_string = file.read().replace('\\n', '')\n",
    "    prof_tokens = prof_string.split(\", \")\n",
    "    for token in prof_tokens:\n",
    "        profanities.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "favorite-northern",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123794it [03:20, 617.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for s,song in tqdm(enumerate(lyric_corpus_tokenized)):\n",
    "    filtered_text = []    \n",
    "    for token in song:\n",
    "        if token not in profanities:\n",
    "            filtered_text.append(token)\n",
    "    lyric_corpus_tokenized[s] = filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "casual-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 13527321 tokens in the current corpus.\n"
     ]
    }
   ],
   "source": [
    "count_token_in_corpus(lyric_corpus_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "attempted-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally store tokenized lyrics \n",
    "fileObject = open(\"./data/lyric_corpus_tokenized\",'wb') \n",
    "pickle.dump(lyric_corpus_tokenized,fileObject)   \n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-closure",
   "metadata": {},
   "source": [
    "## Step 6: Dictionary Creation and Filtering\n",
    "\n",
    "A dictionary representation of the lyrics is created (mapping all tokens to a unique ID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "speaking-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens:  132588\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(lyric_corpus_tokenized)\n",
    "print('Number of unique tokens: ', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-rehabilitation",
   "metadata": {},
   "source": [
    "In order to further reduce dimensionality, tokens that occur less than 100 songs, as well as tokens that occur in more than 80% of songs are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "sweet-ownership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens:  5638\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below = 100, no_above = 0.8)\n",
    "print('Number of unique tokens: ', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-cisco",
   "metadata": {},
   "source": [
    "## Step 7: Bag-of-Words and Index to Dictionary Conversion\n",
    "\n",
    "Each song (as of now a list of tokens) is converted into the bag-of-words format, which only stores the unique token ID and its count for each song.\n",
    "<br>\n",
    "<font color='red'> All preprocessing should be done before this step! </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "residential-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import MmCorpus\n",
    "\n",
    "gensim_corpus = [dictionary.doc2bow(song) for song in lyric_corpus_tokenized]\n",
    "\n",
    "#create index to dictionary\n",
    "temp = dictionary[0]  # \"loads\" the dictionary\n",
    "id2word = dictionary.id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-pierce",
   "metadata": {},
   "source": [
    "## Step 8: Setting the Model Parameters\n",
    "\n",
    "Before commencing training, the models' parameters have to be set. \n",
    "From *gensim* documentation:\n",
    "- *chunksize* = the number of documents considered in each training cycle\n",
    "- *passes* = number of passes through the corpus during training\n",
    "- *iterations* = maximum number of iterations\n",
    "- *start, limit, step* = for which number of topics (which range) should the models be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "superb-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "num_topics = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-pickup",
   "metadata": {},
   "source": [
    "## Step 9: Execute Training and calculate coherence\n",
    "\n",
    "We train a LDA topic model for number of topics k (num_topics). Due to the large song corpus this may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "departmental-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "lda_model = LdaModel(\n",
    "corpus=gensim_corpus,\n",
    "id2word=id2word,\n",
    "chunksize=chunksize,\n",
    "alpha='auto',\n",
    "eta='auto',\n",
    "iterations=iterations,\n",
    "num_topics=num_topics,\n",
    "passes=passes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-bloom",
   "metadata": {},
   "source": [
    "Calculate Cv coherence score (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "intense-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "#coherencemodel = CoherenceModel(model=lda_model, texts=lyric_corpus_tokenized, dictionary=dictionary, coherence='c_v')\n",
    "#print(coherencemodel.get_coherence())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-storage",
   "metadata": {},
   "source": [
    "## Step 10: Visualize the LDA model using pyLDAvis\n",
    "Visualize and store the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "regulated-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "vis_data = gensimvis.prepare(lda_model, gensim_corpus, dictionary)\n",
    "#pyLDAvis.display(vis_data)\n",
    "pyLDAvis.save_html(vis_data, './Lyrics_LDA_k_'+ str(num_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-marking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
